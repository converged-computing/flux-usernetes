#!/usr/bin/env python3

import argparse
import os
import json
import time
import fnmatch

from metricsoperator.metrics.network.osu_benchmark import (
    parse_barrier_section,
    parse_multi_section,
)
import metricsoperator.utils as utils
import seaborn as sns
import matplotlib.pyplot as plt
import pandas

plt.style.use("bmh")
here = os.path.dirname(os.path.abspath(__file__))


def get_parser():
    parser = argparse.ArgumentParser(
        description="Run OSU Benchmarks Metric and Get Output",
        formatter_class=argparse.RawTextHelpFormatter,
    )
    parser.add_argument(
        "--results",
        help="directory with raw results data",
        default=os.path.join(here, "final", "results-osu"),
    )
    parser.add_argument(
        "--out",
        help="directory to save parsed results",
        default=os.path.join(here, "img"),
    )
    return parser


def recursive_find(base, pattern="*.*"):
    """
    Recursively find and yield files matching a glob pattern.
    """
    for root, _, filenames in os.walk(base):
        for filename in fnmatch.filter(filenames, pattern):
            yield os.path.join(root, filename)


def find_inputs(input_dir):
    """
    Find inputs (results files)
    """
    files = []
    for filename in recursive_find(input_dir, pattern="*.out"):
        # We only have data for small
        files.append(filename)
    return files


def main():
    """
    Run the main plotting operation!
    """
    parser = get_parser()
    args, _ = parser.parse_known_args()

    # Output images and data
    outdir = os.path.abspath(args.out)
    indir = os.path.abspath(args.results)
    if not os.path.exists(outdir):
        os.makedirs(outdir)

    # Find input files (skip anything with test)
    files = find_inputs(indir)
    if not files:
        raise ValueError(f"There are no input files in {indir}")

    # This does the actual parsing of data into a formatted variant
    dfs = parse_data(files)
    for key, df in dfs.items():
        df.to_csv(os.path.join(outdir, f"osu-results-{key}.csv"))
    plot_results(dfs, outdir)


def plot_results(dfs, img):
    """
    Plot result images to file
    """
    # Save each completed data frame to file and plot!
    for slug, df in dfs.items():
        print(slug)
        if slug == "latency":
            combined = df.groupby(["experiment", "size"]).mean()
            combined.to_csv(os.path.join(img, "osu-latency-only.csv"))
            combined_nodes = df.groupby(["experiment", "nodes", "size"]).mean()
            combined_nodes.to_csv(os.path.join(img, "osu-latency-node-sizes.csv"))
        if "size" in df.columns:
            print(df.groupby(["experiment", "nodes", "size"]).mean())
            print(df.groupby(["experiment", "nodes", "size"]).std())
        else:
            print(df.groupby(["experiment", "nodes"]).mean())
            print(df.groupby(["experiment", "nodes"]).std())

    return
    # Save each completed data frame to file and plot!
    for slug, df in dfs.items():
        # Barrier we can show across nodes
        if slug == "barrier":
            # Remove usernetes from the set
            without_usernetes = df[df.experiment != "usernetes"]

            # Separate x and y - latency (y) is a function of size (x)
            x = "ranks"
            y = "average_latency_us"

            # for sty in plt.style.available:
            ax = sns.lineplot(
                data=df,
                x=x,
                y=y,
                markers=True,
                dashes=True,
                errorbar=("ci", 95),
                hue="experiment",
                palette="Set1",
            )
            plt.title(f"{slug} (y) as a function of size (x) across nodes")
            ax.set_xlabel("size (logscale)", fontsize=16)
            ax.set_ylabel("Average latency (logscale)", fontsize=16)
            ax.set_xticklabels(ax.get_xmajorticklabels(), fontsize=14)
            ax.set_yticklabels(ax.get_yticks(), fontsize=14)
            plt.subplots_adjust(left=0.2, bottom=0.2)
            plt.xscale("log")
            plt.yscale("log")
            plt.savefig(os.path.join(img, f"osu-{slug}-across-nodes.png"))
            plt.clf()
            plt.close()

            ax = sns.lineplot(
                data=without_usernetes,
                x=x,
                y=y,
                markers=True,
                dashes=True,
                errorbar=("ci", 95),
                hue="experiment",
                palette="Set1",
            )
            plt.title(f"{slug} (y) as a function of size (x) across nodes")
            ax.set_xlabel("size (logscale)", fontsize=16)
            ax.set_ylabel("Average latency (logscale)", fontsize=16)
            ax.set_xticklabels(ax.get_xmajorticklabels(), fontsize=14)
            ax.set_yticklabels(ax.get_yticks(), fontsize=14)
            plt.xscale("log")
            plt.yscale("log")
            plt.subplots_adjust(left=0.2, bottom=0.2)
            plt.savefig(
                os.path.join(img, f"osu-{slug}-across-nodes-without-usernetes.png")
            )
            plt.clf()
            plt.close()
            continue

        for nodes in df.nodes.unique():
            print(f"Preparing plot for {slug}")

            subset = df[df.nodes == nodes]

            # Remove usernetes from the set
            without_usernetes = df[df.experiment != "usernetes"]

            # Separate x and y - latency (y) is a function of size (x)
            x = "size"
            if slug == "barrier":
                x = "ranks"
            y = "average_latency_us"

            # for sty in plt.style.available:
            ax = sns.lineplot(
                data=subset,
                x=x,
                y=y,
                markers=True,
                dashes=True,
                errorbar=("ci", 95),
                hue="experiment",
                palette="Set1",
            )
            plt.title(f"{slug} (y) as a function of {x} (x) on {nodes} nodes")
            ax.set_xlabel(x + " (logscale)", fontsize=16)
            ax.set_ylabel("Average latency (logscale)", fontsize=16)
            ax.set_xticklabels(ax.get_xmajorticklabels(), fontsize=14)
            ax.set_yticklabels(ax.get_yticks(), fontsize=14)
            plt.subplots_adjust(left=0.2, bottom=0.2)
            plt.xscale("log")
            plt.yscale("log")
            plt.savefig(os.path.join(img, f"osu-{slug}-{nodes}-nodes.png"))
            plt.clf()
            plt.close()

            ax = sns.lineplot(
                data=without_usernetes,
                x=x,
                y=y,
                markers=True,
                dashes=True,
                errorbar=("ci", 95),
                hue="experiment",
                palette="Set1",
            )
            plt.title(f"{slug} (y) as a function of {x} (x) on {nodes} nodes")
            ax.set_xlabel(x + " (logscale)", fontsize=16)
            ax.set_ylabel("Average latency (logscale)", fontsize=16)
            ax.set_xticklabels(ax.get_xmajorticklabels(), fontsize=14)
            ax.set_yticklabels(ax.get_yticks(), fontsize=14)
            plt.xscale("log")
            plt.yscale("log")
            plt.subplots_adjust(left=0.2, bottom=0.2)
            plt.savefig(
                os.path.join(img, f"osu-{slug}-{nodes}-nodes-without-usernetes.png")
            )
            plt.clf()
            plt.close()


def parse_data(files):
    """
    Given a listing of files, parse into results data frame
    """
    # Parse into data frames for type
    df_barrier = pandas.DataFrame(
        columns=[
            "ranks",
            "experiment",
            "iteration",
            "average_latency_us",
            "nodes",
        ]
    )
    barrier_idx = 0

    df_latency = pandas.DataFrame(
        columns=[
            "ranks",
            "experiment",
            "iteration",
            "size",
            "average_latency_us",
            "nodes",
        ]
    )
    latency_idx = 0

    df_reduce = pandas.DataFrame(
        columns=[
            "ranks",
            "experiment",
            "iteration",
            "size",
            "average_latency_us",
            "nodes",
        ]
    )
    reduce_idx = 0

    for filename in files:
        parsed = os.path.relpath(filename, here)
        pieces = parsed.split(os.sep)
        experiment = pieces[-2]
        filebase = pieces[-1]

        # barrier and allreduce, ranks are relevant
        if "all_reduce" in filename:
            iteration = int(filebase.split("-")[-1].replace(".out", ""))
            nodes = int(filebase.split("-")[-3])
            tasks = int(filebase.split("-")[-2])
        elif "all-reduce" in filename:
            iteration = int(filebase.split("-")[-1].replace(".out", ""))
            nodes = int(filebase.split("-")[-2])
            tasks = nodes * 16
        elif "osu_barrier" in filename:
            iteration = int(filebase.split("-")[-1].replace(".out", ""))
            nodes = int(filebase.split("-")[-3])
            tasks = int(filebase.split("-")[-2])
        elif "osu-barrier" in filename:
            iteration = int(filebase.split("-")[-1].replace(".out", ""))
            nodes = int(filebase.split("-")[-2])
            tasks = nodes * 16
        else:
            iteration = int(filebase.split("-")[-1].replace(".out", ""))
            nodes = 2
            tasks = 2

        # Save CPU line
        # This is a list, each a json result, 20x
        item = utils.read_file(filename)

        # Full command is the first item
        if "barrier" in filename:
            result = float(item.strip().split("\n")[-1].strip())
            df_barrier.loc[barrier_idx, :] = [
                tasks,
                experiment,
                iteration,
                result,
                nodes,
            ]
            barrier_idx += 1
            continue

        if "latency" in filename:
            result = parse_multi_section(item.split("\n"))
            for row in result["matrix"]:
                df_latency.loc[latency_idx, :] = [
                    tasks,
                    experiment,
                    iteration,
                    row[0],
                    row[1],
                    nodes,
                ]
                latency_idx += 1
            continue

        if "reduce" in filename:
            result = parse_multi_section([x for x in item.split("\n") if x])
            for row in result["matrix"]:
                df_reduce.loc[reduce_idx, :] = [
                    tasks,
                    experiment,
                    iteration,
                    row[0],
                    row[1],
                    nodes,
                ]
                reduce_idx += 1

    dfs = {"all_reduce": df_reduce, "latency": df_latency, "barrier": df_barrier}
    return dfs


if __name__ == "__main__":
    main()
